# LSTM 과 Transformer의 구조적 차이

## 1. LSTM과 Transformer의 구조적 차이

### 1. LSTM (Long Short-Term Memory) - RNN(순환신경망)의 개선버전

- 시퀀스 : 순서가 중요한 데이터의 나열

- 순차적 처리 : 데이터를 과거부터 전달받으며, 은닉상태와 게이트 구조를 통해 의존성을 학습
  - 은닉 상태 : 신경망 내부를 외부에서 직접 관찰 할 수 없는 상태를 유지하며 정보를 처리
  - 게이트 구조 : Input / Forget / Output 게이트를 사용해서 사용할 정보를 결정

|                      장점                      |                             단점                              |
| :--------------------------------------------: | :-----------------------------------------------------------: |
|                시계열 특화 구조                |                  긴 시퀀스에서 속도가 느려짐                  |
| 구조가 직관적이며, 중소형 규모의 시계열에 적합 | 매우 긴 시계열에선 장기 의존성이 제대로 학습 되지않을 수 있음 |

### 2. Transformer

- 병렬처리 : 모든 시점의 데이터를 Self-Attention으로 한번에 처리하므로, 학습 속도가 빠르고 병렬화가 용이함.
- Self-Attension : 각 토큰이 다른 토큰과의 상호작용을 학습해, 의존성을 쉽게 포착

|                                     장점                                     |                         단점                          |
| :--------------------------------------------------------------------------: | :---------------------------------------------------: |
|                    긴 시퀀스 에서도 효율적으로 학습 가능                     | 구조가 복잡하며, 파라미터 수가 많아 메모리 소모가 큼  |
| 여러개의 어텐션을 동시에 적용함으로 다양한 관점에서 시계열의 상관관계를 파악 | 작은 규모의 데이터나, 짧은 시퀀스에는 과도 할 수 있음 |
